{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: gensim in /opt/conda/lib/python3.7/site-packages (4.1.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding tokens:  15173\n",
      "After adding tokens:  15175\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch.utils.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save('brown.embedding')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "print(\"Before adding tokens: \", len(w2v.wv))\n",
    "w2v.wv.add_vectors(['SOS','EOS'],[np.zeros(100),np.zeros(100)])\n",
    "print(\"After adding tokens: \", len(w2v.wv))\n",
    "\n",
    "embedding_weights = torch.FloatTensor(w2v.wv.vectors)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('qa_Home_and_Kitchen.json.gz')\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    tokens.append('EOS')\n",
    "    tokens.insert(0,'SOS')\n",
    "    for x in range(10-len(tokens)):\n",
    "        tokens += '0'\n",
    "        \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def filter_pairs(src, trg):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return len(tokenizer.tokenize(src)) < 8 and len(tokenizer.tokenize(trg)) < 8\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(SRC, TRG):\n",
    "    \n",
    "    tf = []\n",
    "    for x in zip(SRC, TRG):\n",
    "        if filter_pairs(x[0],x[1]):\n",
    "            tf.append(True)\n",
    "        else:\n",
    "            tf.append(False)\n",
    "    \n",
    "    SRC = SRC[tf]\n",
    "    TRG = TRG[tf]\n",
    "            \n",
    "    \n",
    "    SRC_clean = [prepare_text(sentence) for sentence in SRC]\n",
    "    TRG_clean = [prepare_text(sentence) for sentence in TRG]\n",
    "    \n",
    "    SRC_train_size = int(0.8 * len(SRC_clean))\n",
    "    SRC_test_size = len(SRC_clean) - SRC_train_size\n",
    "    SRC_train_dataset, SRC_test_dataset = torch.utils.data.random_split(SRC_clean, [SRC_train_size, SRC_test_size])\n",
    "    \n",
    "    TRG_train_size = int(0.8 * len(TRG_clean))\n",
    "    TRG_test_size = len(TRG_clean) - TRG_train_size\n",
    "    TRG_train_dataset, TRG_test_dataset = torch.utils.data.random_split(TRG_clean, [TRG_train_size, TRG_test_size])\n",
    "    \n",
    "    return SRC_train_dataset, SRC_test_dataset, TRG_train_dataset, TRG_test_dataset\n",
    "\n",
    "SRC_train_dataset, SRC_test_dataset, TRG_train_dataset, TRG_test_dataset = train_test_split(df['question'],df['answer'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.hidden = torch.zeros(1, 1, hidden_size)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, 100) # 100 corresponds to the embedding dimension\n",
    "        self.embedding = self.embedding.from_pretrained(embedding_weights, freeze=False)\n",
    "        \n",
    "        self.lstm = nn.LSTM(100, self.hidden_size, 1) \n",
    "        \n",
    "    \n",
    "    def forward(self, i):\n",
    "        \n",
    "        o, (h, c) = self.lstm(self.embedding(i)) # i.view(1,1, self.input_size), self.embedding(i.view(1,1,self.input_size))\n",
    "        \n",
    "        return o, h, c\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.hidden_size, self.hidden_size)\n",
    "        self.embedding = self.embedding.from_pretrained(embedding_weights, freeze=False)\n",
    "        \n",
    "        self.lstm = nn.LSTM(100, self.hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, i, h):\n",
    "        \n",
    "        output = self.embedding(i).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, h = self.lstm(output, h)\n",
    "        output = self.out(output.squeeze(0))\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, h\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.hidden1 = (torch.zeros(1, 1, encoder_hidden_size), torch.zeros(1, 1, encoder_hidden_size)) #(torch.zeros(1, 1, 20),torch.zeros(1, 1, 20))\n",
    "        \n",
    "        self.encoder = Encoder(encoder_input_size, encoder_hidden_size)\n",
    "        \n",
    "        self.hidden2 = (torch.zeros(1, 1, decoder_hidden_size),\n",
    "                        torch.zeros(1, 1, decoder_hidden_size))\n",
    "        \n",
    "        self.decoder = Decoder(decoder_hidden_size,decoder_output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, src, trg, train=False):\n",
    "        \n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        i = trg[0,:]\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            output, (hidden, cell) = self.decoder(i, (hidden, cell))\n",
    "            \n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            \n",
    "            if train:\n",
    "                i = trg[t]\n",
    "            else:\n",
    "                i = top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "def get_vector_index(word):\n",
    "    \n",
    "    try:\n",
    "        val = w2v.wv.key_to_index[word]\n",
    "    except:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "def training_sentence_tensors(SRC, TRG):\n",
    "    \n",
    "    src_sentence_index = [get_vector_index(word) for word in SRC]\n",
    "    trg_sentence_index = [get_vector_index(word) for word in TRG]\n",
    "    src_tensor = torch.tensor(src_sentence_index, dtype=torch.long).view(-1, 1)\n",
    "    trg_tensor = torch.tensor(trg_sentence_index, dtype=torch.long).view(-1, 1)\n",
    "    return (src_tensor, trg_tensor)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch Loss:  tensor(2.2102, grad_fn=<DivBackward0>)\n",
      "Epoch:  1\n",
      "Epoch Loss:  tensor(1.9300, grad_fn=<DivBackward0>)\n",
      "Epoch:  2\n",
      "Epoch Loss:  tensor(1.8259, grad_fn=<DivBackward0>)\n",
      "Epoch:  3\n",
      "Epoch Loss:  tensor(1.7644, grad_fn=<DivBackward0>)\n",
      "Epoch:  4\n",
      "Epoch Loss:  tensor(1.7217, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "\n",
    "\n",
    "model = Seq2Seq(10, 20, 20, len(w2v.wv))\n",
    "model.apply(init_weights)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    h = model.hidden1\n",
    "    loss_avg = 0\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for i in range(len(SRC_train_dataset)):\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        src, trg = training_sentence_tensors(SRC_train_dataset[i], TRG_train_dataset[i])\n",
    "        \n",
    "        o = model(src, trg, train=True)\n",
    "        \n",
    "        o_dim = o.shape[-1]\n",
    "        \n",
    "        o = o[1:].view(-1, o_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "                \n",
    "        loss = loss_function(o, trg)\n",
    "        loss_avg += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch Loss: \", loss_avg/len(SRC_train_dataset))\n",
    "    loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hi\n",
      "HomeHelper:  the Yes EOS 0 0 0 0 0 0 0\n",
      "You: What can you do?\n",
      "HomeHelper:  the Yes EOS 0 0 0 0 0 0 0\n",
      "You: Help m\n",
      "HomeHelper:  the Yes EOS 0 0 0 0 0 0 0\n",
      "You: Bye\n",
      "HomeHelper:  the Yes EOS 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-98487421b46c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0;31m#  print(e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0;31m#  print(\"Error: Unknown vocabulary word.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-98487421b46c>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m        \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_response(sentence):\n",
    "    \n",
    "    sentence = prepare_text(sentence)\n",
    "    sentence_index = [get_vector_index(word) for word in sentence]\n",
    "    tensor = torch.tensor(sentence_index, dtype=torch.long).view(-1, 1)\n",
    "    response = model.forward(tensor, tensor)\n",
    "    \n",
    "    response = [int(o.argmax(1)[0]) for o in model(tensor, tensor)]\n",
    "    response = [w2v.wv.index_to_key[x] for x in response]\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def chat():\n",
    "    \n",
    "    while(True):\n",
    "       # try:\n",
    "            \n",
    "            sentence = input(\"You: \")\n",
    "            \n",
    "            if sentence == 'exit':\n",
    "                break\n",
    "            else:\n",
    "                response = generate_response(sentence)\n",
    "                print(\"HomeHelper: \", ' '.join(response) )\n",
    "                \n",
    "        # except Exception as e:\n",
    "          #  print(e)\n",
    "          #  print(\"Error: Unknown vocabulary word.\")\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
